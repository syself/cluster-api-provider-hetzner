---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: "${CLUSTER_NAME}"
spec:
  clusterNetwork:
    pods:
      cidrBlocks: 
        - 10.244.0.0/16
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: HetznerCluster
    name: "${CLUSTER_NAME}"
  controlPlaneRef:
    kind: KubeadmControlPlane
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    name: "${CLUSTER_NAME}-control-plane"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: HetznerCluster
metadata:
  name: "${CLUSTER_NAME}"
spec:
  hcloudNetwork:
    enabled: true
  controlPlaneRegions: 
    - "${REGION}"
  controlPlaneEndpoint:
    host: ""
    port: 443
  controlPlaneLoadBalancer:
    region: "${REGION}"
  hcloudPlacementGroups:
  - name: control-plane
    type: spread
  - name: md-0
    type: spread
  sshKeys:
    hcloud:
    - name: "${SSH_KEY}"
  hetznerSecretRef:
    name: hetzner
    key:
      hcloudToken: hcloud
---
kind: KubeadmControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: "${CLUSTER_NAME}-control-plane"
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  machineTemplate:
    infrastructureRef:
      kind: HCloudMachineTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      name: "${CLUSTER_NAME}-control-plane"
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          # CA certificate for validating API clients.
          client-ca-file: /etc/kubernetes/pki/ca.crt # enable X509 Client Certs Auth
          # TLS certificates for HTTPS serving.
          tls-cert-file: /etc/kubernetes/pki/apiserver.crt
          tls-private-key-file: /etc/kubernetes/pki/apiserver.key
          tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
          # Required for kubelet communication.
          kubelet-client-certificate: /etc/kubernetes/pki/apiserver-kubelet-client.crt
          kubelet-client-key: /etc/kubernetes/pki/apiserver-kubelet-client.key
          # Secure communication to etcd servers.
          etcd-cafile: /etc/kubernetes/pki/etcd/ca.crt
          etcd-certfile: /etc/kubernetes/pki/etcd/server.crt
          etcd-keyfile: /etc/kubernetes/pki/etcd/server.key
          # Required to validate service account tokens created by controller manager.
          service-account-lookup: "true"
          service-account-key-file: /etc/kubernetes/pki/sa.pub
          # Required for aggregation layer
          requestheader-client-ca-file: /etc/kubernetes/pki/front-proxy-ca.crt
          proxy-client-key-file: /etc/kubernetes/pki/front-proxy-client.key
          proxy-client-cert-file: /etc/kubernetes/pki/front-proxy-client.crt
          requestheader-allowed-names: front-proxy-client
          requestheader-extra-headers-prefix: X-Remote-Extra-
          requestheader-group-headers: X-Remote-Group
          requestheader-username-headers: X-Remote-User
          enable-aggregator-routing: "true"
          # Etcd Secret Encrytion
          encryption-provider-config: /etc/kubernetes/encryption-provider.yaml
          # Additional Configuration
          cloud-provider: external
          authorization-mode: "Node,RBAC"
          kubelet-preferred-address-types: ExternalIP,Hostname,InternalDNS,ExternalDNS
          profiling: "false"
          enable-bootstrap-token-auth: "true"
          insecure-port: "0"
          default-not-ready-toleration-seconds: "45"
          default-unreachable-toleration-seconds: "45"
        extraVolumes:
        - name: encryption-provider
          hostPath: /etc/kubernetes/encryption-provider.yaml
          mountPath: /etc/kubernetes/encryption-provider.yaml
      controllerManager:
        extraArgs:
          cloud-provider: external
          cluster-signing-cert-file: /etc/kubernetes/pki/ca.crt
          cluster-signing-key-file: /etc/kubernetes/pki/ca.key
          cluster-signing-duration: 1h35m0s
          terminated-pod-gc-threshold: "10"
          profiling: "false"
          use-service-account-credentials: "true"
          service-account-private-key-file: /etc/kubernetes/pki/sa.key
          root-ca-file: /etc/kubernetes/pki/ca.crt
          requestheader-client-ca-file: /etc/kubernetes/pki/front-proxy-ca.crt
          kubeconfig: /etc/kubernetes/controller-manager.conf
          authentication-kubeconfig: /etc/kubernetes/controller-manager.conf
          authorization-kubeconfig: /etc/kubernetes/controller-manager.conf
          address: "127.0.0.1"
          bind-address: "0.0.0.0"
          port: "0"
          secure-port: "10257"
          allocate-node-cidrs: "true"
          pod-eviction-timeout: 2m
      scheduler:
        extraArgs:
          profiling: "false"
          kubeconfig: /etc/kubernetes/scheduler.conf
          address: "127.0.0.1"
          bind-address: "0.0.0.0"
          port: "0"
          secure-port: "10259"
      etcd:
        local:
          dataDir: /var/lib/etcd
          extraArgs:
            cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
            cert-file: /etc/kubernetes/pki/etcd/server.crt
            key-file: /etc/kubernetes/pki/etcd/server.key
            client-cert-auth: "true"
            auto-tls: "false"
            peer-client-cert-auth: "true"
            peer-auto-tls: "false"
            trusted-ca-file: /etc/kubernetes/pki/etcd/ca.crt
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
          cgroup-driver: systemd
          tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
          kubeconfig: /etc/kubernetes/kubelet.conf
          authentication-token-webhook: "true"
          authorization-mode: Webhook
          anonymous-auth: "false"
          read-only-port: "0"
          event-qps: "5"
          rotate-server-certificates: "true"
          max-pods: "120"
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
          cgroup-driver: systemd
          tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
          kubeconfig: /etc/kubernetes/kubelet.conf
          authentication-token-webhook: "true"
          authorization-mode: Webhook
          anonymous-auth: "false"
          read-only-port: "0"
          event-qps: "5"
          rotate-server-certificates: "true"
          max-pods: "120"
    files:
      - path: /etc/kubernetes/encryption-provider.yaml
        owner: "root:root"
        permissions: "0600"
        content: |
          apiVersion: apiserver.config.k8s.io/v1
          kind: EncryptionConfiguration
          resources:
            - resources:
              - secrets
              providers:
              - aescbc:
                  keys:
                  - name: key1
                    secret: 8d7iAcg3/NwN9aijhtEXj5kL2NOHIgokGFjbIBfL6X0=
              - identity: {}
      - path: /etc/systemd/system/sys-fs-bpf.mount
        owner: "root:root"
        permissions: "0744"
        content: |
          [Unit]
          Description=Cilium BPF mounts
          Documentation=https://docs.cilium.io/
          DefaultDependencies=no
          Before=local-fs.target umount.target
          After=swap.target

          [Mount]
          What=bpffs
          Where=/sys/fs/bpf
          Type=bpf
          Options=rw,nosuid,nodev,noexec,relatime,mode=700

          [Install]
          WantedBy=multi-user.target
      - path: /etc/sysctl.d/99-cilium.conf
        owner: "root:root"
        permissions: "0744"
        content: |
          net.ipv4.conf.lxc*.rp_filter = 0
      - path: /etc/modules-load.d/crio.conf
        owner: "root:root"
        permissions: "0744"
        content: |
          overlay
          br_netfilter
      - path: /etc/crio/crio.conf.d/02-cgroup-manager.conf
        owner: "root:root"
        permissions: "0744"
        content: |
          [crio]
          log_dir = "/var/log/crio/pods"
          grpc_max_send_msg_size = 16777216
          grpc_max_recv_msg_size = 16777216
          [crio.runtime]
          default_runtime = "runc"
          conmon = "/usr/local/bin/conmon"
          conmon_env = [
              "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
          ]
          default_env = [
          ]
          selinux = false
          seccomp_profile = ""
          apparmor_profile = "crio-default"
          default_capabilities = [
            "CHOWN",
            "DAC_OVERRIDE",
            "FSETID",
            "FOWNER",
            "SETGID",
            "SETUID",
            "SETPCAP",
            "NET_BIND_SERVICE",
            "KILL",
              "MKNOD",
          ]
          [crio.runtime.runtimes.runc]
          runtime_path = ""
          runtime_type = "oci"
          runtime_root = "/run/runc"
      - path: /etc/yum.repos.d/kubernetes.repo
        owner: "root:root"
        permissions: "0744"
        content: |
          [kubernetes]
          name=Kubernetes
          baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
          enabled=1
          gpgcheck=1
          repo_gpgcheck=1
          gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
      - path: /etc/sysctl.d/99-kubernetes-cri.conf
        owner: "root:root"
        permissions: "0744"
        content: |
          net.bridge.bridge-nf-call-iptables  = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          net.ipv4.ip_forward                 = 1
      - path: /etc/sysctl.d/99-kubelet.conf
        owner: "root:root"
        permissions: "0744"
        content: |
          vm.overcommit_memory=1
          kernel.panic=10
          kernel.panic_on_oops=1
    preKubeadmCommands:
      - localectl set-locale LANG=en_US.UTF-8
      - localectl set-locale LANGUAGE=en_US.UTF-8
      - dnf update -y
      - dnf -y install at jq unzip wget socat mtr firewalld
      - sed -i '/swap/d' /etc/fstab
      - swapoff -a
      - modprobe overlay && modprobe br_netfilter && sysctl --system
      - wget https://github.com/opencontainers/runc/releases/download/v1.1.0/runc.amd64 -O /usr/local/sbin/runc && chmod +x /usr/local/sbin/runc
      - wget https://github.com/containers/conmon/releases/download/v2.1.0/conmon-x86.zip -O conmon.zip && unzip conmon.zip -d conmon && mv conmon/bin/conmon /usr/local/bin/conmon && chmod +x /usr/local/bin/conmon && rm -rf conmon.zip conmon
      - curl https://raw.githubusercontent.com/cri-o/cri-o/main/scripts/get | bash -s -- -t v1.23.0 -a amd64
      - wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.23.0/crictl-v1.23.0-linux-amd64.tar.gz && tar zxvf crictl-v1.23.0-linux-amd64.tar.gz -C /usr/local/bin && rm -f crictl-v1.23.0-linux-amd64.tar.gz
      - rm -f /etc/cni/net.d/100-crio-bridge.conf /etc/cni/net.d/200-loopback.conf
      - systemctl enable crio.service && systemctl daemon-reload && systemctl enable crio
      - dnf install --setopt=obsoletes=0 -y kubelet-0:1.23.3-0 kubeadm-0:1.23.3-0 kubectl-0:1.23.3-0 python3-dnf-plugin-versionlock bash-completion --disableexcludes=kubernetes && dnf versionlock kubelet kubectl kubeadm && systemctl enable kubelet && systemctl start crio && kubeadm config images pull --kubernetes-version 1.23.3
      - dnf install -y policycoreutils-python-utils
      - semanage fcontext -a -t container_file_t /var/lib/etcd && mkdir -p /var/lib/etcd && restorecon -rv /var /etc
      - echo 'source <(kubectl completion bash)' >>~/.bashrc
      - echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' >>~/.bashrc
      - setenforce 0 && sed -i -e '/^\(#\|\)SELINUX/s/^.*$/SELINUX=disabled/' /etc/selinux/config
      - dnf -y remove firewalld
      - dnf -y autoremove && dnf -y clean all
  version: "${KUBERNETES_VERSION}"
---
kind: HCloudMachineTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
metadata:
  name: "${CLUSTER_NAME}-control-plane"
spec:
  template:
    spec:
      type: "${HCLOUD_CONTROL_PLANE_MACHINE_TYPE}"
      imageName: "fedora-35"
      placementGroupName: control-plane
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: "${CLUSTER_NAME}-control-plane-unhealthy-5m"
spec:
  clusterName: "${CLUSTER_NAME}"
  maxUnhealthy: 100%
  nodeStartupTimeout: 20m
  selector:
    matchLabels:
      cluster.x-k8s.io/control-plane: ""
  unhealthyConditions:
    - type: Ready
      status: Unknown
      timeout: 300s
    - type: Ready
      status: "False"
      timeout: 300s
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "${CLUSTER_NAME}-md-0"
  labels:
    nodepool: "${CLUSTER_NAME}-md-0"
spec:
  clusterName: "${CLUSTER_NAME}"
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels:
  template:
    spec:
      clusterName: "${CLUSTER_NAME}"
      failureDomain: "${REGION}"
      version: "${KUBERNETES_VERSION}"
      bootstrap:
        configRef:
          name: "${CLUSTER_NAME}-md-0"
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
      infrastructureRef:
        name: "${CLUSTER_NAME}-md-0"
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: HCloudMachineTemplate
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: HCloudMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
spec:
  template:
    spec:
      type: "${HCLOUD_NODE_MACHINE_TYPE}"
      imageName: "fedora-35"
      placementGroupName: md-0
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
            cgroup-driver: systemd
            tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256
            kubeconfig: /etc/kubernetes/kubelet.conf
            authentication-token-webhook: "true"
            authorization-mode: Webhook
            anonymous-auth: "false"
            read-only-port: "0"
            event-qps: "5"
            rotate-server-certificates: "true"
            max-pods: "220"
      files:
        - path: /etc/systemd/system/sys-fs-bpf.mount
          owner: "root:root"
          permissions: "0744"
          content: |
            [Unit]
            Description=Cilium BPF mounts
            Documentation=https://docs.cilium.io/
            DefaultDependencies=no
            Before=local-fs.target umount.target
            After=swap.target

            [Mount]
            What=bpffs
            Where=/sys/fs/bpf
            Type=bpf
            Options=rw,nosuid,nodev,noexec,relatime,mode=700

            [Install]
            WantedBy=multi-user.target
        - path: /etc/sysctl.d/99-cilium.conf
          owner: "root:root"
          permissions: "0744"
          content: |
            net.ipv4.conf.lxc*.rp_filter = 0
        - path: /etc/modules-load.d/crio.conf
          owner: "root:root"
          permissions: "0744"
          content: |
            overlay
            br_netfilter
        - path: /etc/crio/crio.conf.d/02-cgroup-manager.conf
          owner: "root:root"
          permissions: "0744"
          content: |
            [crio.runtime]
            default_runtime = "runc"
            conmon = "/usr/local/bin/conmon"
            conmon_env = [
                "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
            ]
            selinux = false
            seccomp_profile = ""
            apparmor_profile = "crio-default"
            default_capabilities = [
              "CHOWN",
              "DAC_OVERRIDE",
              "FSETID",
              "FOWNER",
              "SETGID",
              "SETUID",
              "SETPCAP",
              "NET_BIND_SERVICE",
              "KILL",
              "MKNOD",
            ]
            [crio.runtime.runtimes.runc]
            runtime_path = ""
            runtime_type = "oci"
            runtime_root = "/run/runc"
        - path: /etc/yum.repos.d/kubernetes.repo
          owner: "root:root"
          permissions: "0744"
          content: |
            [kubernetes]
            name=Kubernetes
            baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
            enabled=1
            gpgcheck=1
            repo_gpgcheck=1
            gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
        - path: /etc/sysctl.d/99-kubernetes-cri.conf
          owner: "root:root"
          permissions: "0744"
          content: |
            net.bridge.bridge-nf-call-iptables  = 1
            net.bridge.bridge-nf-call-ip6tables = 1
            net.ipv4.ip_forward                 = 1
        - path: /etc/sysctl.d/99-kubelet.conf
          owner: "root:root"
          permissions: "0744"
          content: |
            vm.overcommit_memory=1
            kernel.panic=10
            kernel.panic_on_oops=1
      preKubeadmCommands:
        - localectl set-locale LANG=en_US.UTF-8
        - localectl set-locale LANGUAGE=en_US.UTF-8
        - dnf update -y
        - dnf -y install at jq unzip wget socat mtr firewalld
        - sed -i '/swap/d' /etc/fstab
        - swapoff -a
        - modprobe overlay && modprobe br_netfilter && sysctl --system
        - wget https://github.com/opencontainers/runc/releases/download/v1.1.0/runc.amd64 -O /usr/local/sbin/runc && chmod +x /usr/local/sbin/runc
        - wget https://github.com/containers/conmon/releases/download/v2.1.0/conmon-x86.zip -O conmon.zip && unzip conmon.zip -d conmon && mv conmon/bin/conmon /usr/local/bin/conmon && chmod +x /usr/local/bin/conmon && rm -rf conmon.zip conmon
        - curl https://raw.githubusercontent.com/cri-o/cri-o/main/scripts/get | bash -s -- -t v1.23.0 -a amd64
        - wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.23.0/crictl-v1.23.0-linux-amd64.tar.gz && tar zxvf crictl-v1.23.0-linux-amd64.tar.gz -C /usr/local/bin && rm -f crictl-v1.23.0-linux-amd64.tar.gz
        - rm -f /etc/cni/net.d/100-crio-bridge.conf /etc/cni/net.d/200-loopback.conf
        - systemctl enable crio.service && systemctl daemon-reload && systemctl enable crio
        - dnf install --setopt=obsoletes=0 -y kubelet-0:1.23.3-0 kubeadm-0:1.23.3-0 kubectl-0:1.23.3-0 python3-dnf-plugin-versionlock bash-completion --disableexcludes=kubernetes && dnf versionlock kubelet kubectl kubeadm && systemctl enable kubelet && systemctl start crio && kubeadm config images pull --kubernetes-version 1.23.3
        - dnf install -y policycoreutils-python-utils
        - semanage fcontext -a -t container_file_t /var/lib/etcd && mkdir -p /var/lib/etcd && restorecon -rv /var /etc
        - echo 'source <(kubectl completion bash)' >>~/.bashrc
        - echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' >>~/.bashrc
        - setenforce 0 && sed -i -e '/^\(#\|\)SELINUX/s/^.*$/SELINUX=disabled/' /etc/selinux/config
        - dnf -y remove firewalld
        - dnf -y autoremove && dnf -y clean all
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: "${CLUSTER_NAME}-md-0-unhealthy-5m"
spec:
  clusterName: "${CLUSTER_NAME}"
  maxUnhealthy: 100%
  nodeStartupTimeout: 20m
  selector:
    matchLabels:
      nodepool: "${CLUSTER_NAME}-md-0"
  unhealthyConditions:
    - type: Ready
      status: Unknown
      timeout: 300s
    - type: Ready
      status: "False"
      timeout: 300s